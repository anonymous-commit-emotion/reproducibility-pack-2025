{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import DistilBertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=pd.read_csv('sampled20k.csv')\n",
    "# Load the DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# Keep necessary columns including 'commit'\n",
    "data = raw_data[['commit', 'author', 'date', 'repo', 'message']]\n",
    "\n",
    "# Normalize text (lowercase, strip whitespace, and remove newlines)\n",
    "data['message'] = data['message'].str.lower().str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Remove newlines and replace them with spaces\n",
    "data['message'] = data['message'].str.replace(r'\\s*\\n\\s*', ' ', regex=True)\n",
    "\n",
    "# Remove anything within [...] and <...>\n",
    "data['message'] = data['message'].str.replace(r'\\[.*?\\]', '', regex=True)  # Remove content in square brackets\n",
    "data['message'] = data['message'].str.replace(r'<.*?>', '', regex=True)   # Remove content in angle brackets\n",
    "\n",
    "# Remove two words before <...>\n",
    "data['message'] = data['message'].str.replace(r'\\b\\w+\\s+\\w+\\s+<.*?>', '', regex=True)\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # Remove entire phrases around specific separators\n",
    "    text = re.sub(r'\\b\\w+:\\w+\\b', '', text)  # word1:word2\n",
    "    text = re.sub(r'\\b\\w+=\\w+\\b', '', text)  # word1=word2\n",
    "    text = re.sub(r'\\b\\w+/\\w+\\b', '', text)  # word1/word2\n",
    "    text = re.sub(r'\\b\\w+//\\w+\\b', '', text)  # word1//word2\n",
    "    \n",
    "    # Remove extra whitespace and trim\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "data['message'] = data['message'].astype(str)\n",
    "# Apply the cleaning function\n",
    "data['message'] = data['message'].apply(clean_text)\n",
    "\n",
    "# Remove email addresses\n",
    "data['message'] = data['message'].str.replace(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b', '', regex=True)\n",
    "\n",
    "# Remove all other URLs and UUID-like identifiers\n",
    "data['message'] = data['message'].str.replace(\n",
    "    r'\\bhttps?://\\S+|www\\.\\S+\\b|git-svn-id:.*?\\s|svn://\\S+|git://\\S+|url:\\s*[a-f0-9\\-]{36}\\b',\n",
    "    '',\n",
    "    regex=True\n",
    ")\n",
    "\n",
    "# Remove metadata fields and anything following them\n",
    "metadata_fields = ['reviewed-on:', 'commit-queue:', 'change-id:', 'commit-position:']\n",
    "for field in metadata_fields:\n",
    "    data['message'] = data['message'].str.replace(rf'{field}\\s*.*', '', regex=True)\n",
    "\n",
    "# Retain the word \"bug\" but remove anything after it\n",
    "data['message'] = data['message'].str.replace(r'\\bbug\\b\\s*=\\s*\\S+', 'bug', regex=True)\n",
    "\n",
    "# Remove greater than (>) and less than (<) signs\n",
    "data['message'] = data['message'].str.replace(r'[<>]', '', regex=True)\n",
    "\n",
    "# Remove metadata like \"signed-off-by,\" \"reviewed-by,\" \"acked-by\"\n",
    "data['message'] = data['message'].str.replace(r'\\b(signed-off-by|reviewed-by|acked-by):\\s+.*?(\\s|$)', '', regex=True)\n",
    "\n",
    "# Remove standalone alphanumeric strings (e.g., abc123, test456)\n",
    "data['message'] = data['message'].str.replace(r'\\b[A-Za-z]*\\d+[A-Za-z]*\\b', '', regex=True)\n",
    "\n",
    "# Remove all numbers (standalone or embedded within words)\n",
    "data['message'] = data['message'].str.replace(r'\\d+', '', regex=True)\n",
    "\n",
    "# Remove strings with more than two repeated letters\n",
    "data['message'] = data['message'].str.replace(r'\\b\\w*(\\w)\\1{2,}\\w*\\b', '', regex=True)\n",
    "\n",
    "# Remove repeated full stops or commas of three or more in a row\n",
    "data['message'] = data['message'].str.replace(r'[.,]{2,}', '', regex=True)\n",
    "\n",
    "# Remove all special characters except for \",\", \"!\", \"?\", \".\", and \"'\"\n",
    "data['message'] = data['message'].str.replace(r\"[^\\w\\s,!?\\.']\", '', regex=True)\n",
    "\n",
    "# Normalize spaces (remove excess spaces left from deletions)\n",
    "data['message'] = data['message'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "# Function to count tokens using the tokenizer\n",
    "def count_tokens(message):\n",
    "    encoding = tokenizer(message, return_tensors='pt', truncation=False, padding=False)\n",
    "    return encoding.input_ids.shape[1]\n",
    "data = data[data['message'].apply(count_tokens) <= 512]  # Keep messages with 512 tokens or fewer\n",
    "\n",
    "# Filter out very short or empty messages\n",
    "data = data[data['message'].str.split().str.len() > 2]  # Keep messages with more than 2 words\n",
    "\n",
    "# Save the preprocessed dataset with 'commit' column retained\n",
    "output_file = \"cleaned20k.csv\"\n",
    "data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Preprocessed dataset saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
