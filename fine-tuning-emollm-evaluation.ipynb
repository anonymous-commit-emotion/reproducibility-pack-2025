{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-26T14:21:24.920069Z",
     "iopub.status.busy": "2025-09-26T14:21:24.919230Z",
     "iopub.status.idle": "2025-09-26T14:21:24.925972Z",
     "shell.execute_reply": "2025-09-26T14:21:24.925136Z",
     "shell.execute_reply.started": "2025-09-26T14:21:24.920034Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-26T14:21:24.928049Z",
     "iopub.status.busy": "2025-09-26T14:21:24.927440Z",
     "iopub.status.idle": "2025-09-26T14:21:24.947965Z",
     "shell.execute_reply": "2025-09-26T14:21:24.947228Z",
     "shell.execute_reply.started": "2025-09-26T14:21:24.928029Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 2. CONFIGURATION ---\n",
    "# This is the only section you need to change between experiments.\n",
    "\n",
    "# --- EXPERIMENT A: Fine-tune the Generalist (RoBERTa-NLI) ---\n",
    "# MODEL_CHECKPOINT = \"FacebookAI/roberta-large-mnli\"\n",
    "# OUTPUT_DIR = \"./results/roberta_nli_finetuned\"\n",
    "\n",
    "# --- EXPERIMENT B: Fine-tune the Specialist (CodeBERT) ---\n",
    "MODEL_CHECKPOINT = \"microsoft/codebert-base\"\n",
    "OUTPUT_DIR = \"./results/codebert_finetuned\"\n",
    "\n",
    "# # --- EXPERIMENT C: Fine-tune the Specialist (distilroberta) ---\n",
    "# MODEL_CHECKPOINT = \"j-hartmann/emotion-english-distioberta-base\"\n",
    "# OUTPUT_DIR = \"./results/roberta_finetuned\"\n",
    "\n",
    "# --- Universal Parameters ---\n",
    "DATA_FILE = \"/kaggle/input/emollm/augmented_training_data_final.csv\" # The path to your 2k labeled dataset\n",
    "TEXT_COLUMN = \"message\"\n",
    "LABEL_COLUMN = \"emotion\"\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-26T14:21:24.948859Z",
     "iopub.status.busy": "2025-09-26T14:21:24.948636Z",
     "iopub.status.idle": "2025-09-26T14:21:24.990458Z",
     "shell.execute_reply": "2025-09-26T14:21:24.989639Z",
     "shell.execute_reply.started": "2025-09-26T14:21:24.948838Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading and Preparing Data ---\n",
      "Dataset loaded successfully. Shape: (4796, 3)\n"
     ]
    }
   ],
   "source": [
    "# --- 3. DATA PREPARATION ---\n",
    "print(\"--- Step 1: Loading and Preparing Data ---\")\n",
    "\n",
    "# Load your dataset\n",
    "try:\n",
    "    aug_df = pd.read_csv(DATA_FILE)\n",
    "    print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL ERROR: Data file not found at '{DATA_FILE}'. Exiting.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T14:21:24.991435Z",
     "iopub.status.busy": "2025-09-26T14:21:24.991162Z",
     "iopub.status.idle": "2025-09-26T14:21:24.996381Z",
     "shell.execute_reply": "2025-09-26T14:21:24.995264Z",
     "shell.execute_reply.started": "2025-09-26T14:21:24.991416Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "aug_df.rename(columns = {'reconciled_emotion': 'emotion', 'reconciled_intensity': 'intensity'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T14:21:24.998726Z",
     "iopub.status.busy": "2025-09-26T14:21:24.998527Z",
     "iopub.status.idle": "2025-09-26T14:21:25.013197Z",
     "shell.execute_reply": "2025-09-26T14:21:25.012540Z",
     "shell.execute_reply.started": "2025-09-26T14:21:24.998710Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['message', 'emotion'], dtype='object')"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T14:21:25.014346Z",
     "iopub.status.busy": "2025-09-26T14:21:25.013989Z",
     "iopub.status.idle": "2025-09-26T14:21:25.032516Z",
     "shell.execute_reply": "2025-09-26T14:21:25.031804Z",
     "shell.execute_reply.started": "2025-09-26T14:21:25.014298Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Enforcing data hygiene ---\n",
      "Found 24 NaN values in 'message' column.\n",
      "Shape after cleaning empty/NaN rows: (4796, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Enforcing data hygiene ---\")\n",
    "# 1. Check for NaN values before cleaning\n",
    "print(f\"Found {aug_df[TEXT_COLUMN].isna().sum()} NaN values in '{TEXT_COLUMN}' column.\")\n",
    "# 2. Convert the entire column to string type to prevent TypeErrors\n",
    "aug_df[TEXT_COLUMN] = aug_df[TEXT_COLUMN].astype(str)\n",
    "# 3. Drop any rows where the message is just whitespace or empty\n",
    "aug_df = aug_df[aug_df[TEXT_COLUMN].str.strip().astype(bool)]\n",
    "print(f\"Shape after cleaning empty/NaN rows: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-26T14:21:25.033635Z",
     "iopub.status.busy": "2025-09-26T14:21:25.033298Z",
     "iopub.status.idle": "2025-09-26T14:21:25.054080Z",
     "shell.execute_reply": "2025-09-26T14:21:25.053110Z",
     "shell.execute_reply.started": "2025-09-26T14:21:25.033608Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping created: {'frustration': 0, 'satisfaction': 1, 'neutral': 2, 'caution': 3}\n"
     ]
    }
   ],
   "source": [
    "# Create the label mappings\n",
    "# The model needs integer labels, not strings.\n",
    "labels = aug_df[LABEL_COLUMN].unique().tolist()\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "NUM_LABELS = len(labels)\n",
    "\n",
    "print(f\"Label mapping created: {label2id}\")\n",
    "\n",
    "# Convert string labels to integer IDs\n",
    "aug_df['labels'] = aug_df[LABEL_COLUMN].map(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-26T14:21:25.055357Z",
     "iopub.status.busy": "2025-09-26T14:21:25.055097Z",
     "iopub.status.idle": "2025-09-26T14:21:25.077694Z",
     "shell.execute_reply": "2025-09-26T14:21:25.076780Z",
     "shell.execute_reply.started": "2025-09-26T14:21:25.055325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_df, val_df = train_test_split(\n",
    "    aug_df,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df['labels'] # Ensures balanced classes in splits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T14:21:25.079143Z",
     "iopub.status.busy": "2025-09-26T14:21:25.078581Z",
     "iopub.status.idle": "2025-09-26T14:21:25.107087Z",
     "shell.execute_reply": "2025-09-26T14:21:25.106372Z",
     "shell.execute_reply.started": "2025-09-26T14:21:25.079118Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "original_df = pd.read_csv(\"/kaggle/input/emollm/labeled_2k.csv\")\n",
    "original_df['labels'] = original_df['reconciled_emotion'].map(label2id)\n",
    "_, test_df = train_test_split(\n",
    "    original_df, test_size=0.2, random_state=RANDOM_STATE, stratify=original_df['labels']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-26T14:21:25.107993Z",
     "iopub.status.busy": "2025-09-26T14:21:25.107810Z",
     "iopub.status.idle": "2025-09-26T14:21:25.132088Z",
     "shell.execute_reply": "2025-09-26T14:21:25.131375Z",
     "shell.execute_reply.started": "2025-09-26T14:21:25.107978Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into training and validation sets.\n",
      "Training set size: 3836\n",
      "Validation set size: 960\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas DataFrames to Hugging Face Dataset objects\n",
    "raw_datasets = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'validation': Dataset.from_pandas(val_df)\n",
    "})\n",
    "\n",
    "print(f\"Data split into training and validation sets.\")\n",
    "print(f\"Training set size: {len(raw_datasets['train'])}\")\n",
    "print(f\"Validation set size: {len(raw_datasets['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-26T14:21:25.134518Z",
     "iopub.status.busy": "2025-09-26T14:21:25.134233Z",
     "iopub.status.idle": "2025-09-26T14:21:26.341386Z",
     "shell.execute_reply": "2025-09-26T14:21:26.340618Z",
     "shell.execute_reply.started": "2025-09-26T14:21:25.134498Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Tokenizing Data ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79422d2011784eb490d32d4ec27c269b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3836 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2fee18e7775452ea75d8926ba2be1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/960 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. TOKENIZATION ---\n",
    "print(\"\\n--- Step 2: Tokenizing Data ---\")\n",
    "\n",
    "# Load the tokenizer for the specific model we are fine-tuning\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[TEXT_COLUMN], truncation=True, padding=False)\n",
    "\n",
    "# Apply the tokenization to the entire dataset\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "# Data collator will handle dynamic padding during batch creation.\n",
    "# This is more efficient than padding all examples to the max length.\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "print(\"Tokenization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-26T14:21:26.342509Z",
     "iopub.status.busy": "2025-09-26T14:21:26.342177Z",
     "iopub.status.idle": "2025-09-26T14:21:26.351154Z",
     "shell.execute_reply": "2025-09-26T14:21:26.350458Z",
     "shell.execute_reply.started": "2025-09-26T14:21:26.342490Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Loading Model and Defining Training Arguments ---\n",
      "Calculated class weights: tensor([0.9917, 0.9293, 0.7243, 1.8694], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# --- 5. MODEL AND TRAINER SETUP ---\n",
    "print(\"\\n--- Step 3: Loading Model and Defining Training Arguments ---\")\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train_df['labels']),\n",
    "    y=train_df['labels'].to_numpy()\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(\"cuda\")\n",
    "print(f\"Calculated class weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T14:21:26.352335Z",
     "iopub.status.busy": "2025-09-26T14:21:26.352063Z",
     "iopub.status.idle": "2025-09-26T14:21:26.366045Z",
     "shell.execute_reply": "2025-09-26T14:21:26.365153Z",
     "shell.execute_reply.started": "2025-09-26T14:21:26.352294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    # THE FIX: Add **kwargs to the function signature to accept and ignore\n",
    "    # any unexpected arguments passed by the Trainer.\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Apply our pre-calculated class weights\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(\n",
    "            weight=class_weights\n",
    "        )\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-26T14:21:26.369175Z",
     "iopub.status.busy": "2025-09-26T14:21:26.368705Z",
     "iopub.status.idle": "2025-09-26T14:21:27.269108Z",
     "shell.execute_reply": "2025-09-26T14:21:27.268324Z",
     "shell.execute_reply.started": "2025-09-26T14:21:26.369155Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4: Loading Model and Defining Training Arguments ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Trainer initialized. Ready to fine-tune.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/3460012697.py:44: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 4: Loading Model and Defining Training Arguments ---\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    num_labels=NUM_LABELS,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Define the evaluation metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average=\"macro\")\n",
    "    return {\"accuracy\": accuracy, \"f1_macro\": f1}\n",
    "\n",
    "\n",
    "# Define the training arguments\n",
    "# These are crucial hyperparameters for your experiment.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=8,                 # A reasonable starting point\n",
    "    per_device_train_batch_size=8,      # Adjust based on GPU memory\n",
    "    per_device_eval_batch_size=8,       # Adjust based on GPU memory\n",
    "    learning_rate=2e-5,                 # A standard learning rate for fine-tuning\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",        # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",              # Save a checkpoint at the end of each epoch\n",
    "    load_best_model_at_end=True,        # The final model will be the best one found\n",
    "    metric_for_best_model=\"f1_macro\",   # Use F1 score to determine the \"best\" model\n",
    "    push_to_hub=False,                  # Set to True to upload to Hugging Face Hub\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=2 # The number of epochs to wait for improvement\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback],\n",
    ")\n",
    "\n",
    "print(\"Model and Trainer initialized. Ready to fine-tune.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-26T14:21:27.270143Z",
     "iopub.status.busy": "2025-09-26T14:21:27.269929Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4: Starting Fine-Tuning ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='887' max='3840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 887/3840 02:24 < 08:00, 6.14 it/s, Epoch 1.85/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.728600</td>\n",
       "      <td>0.854702</td>\n",
       "      <td>0.621875</td>\n",
       "      <td>0.651912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 6. EXECUTE FINE-TUNING ---\n",
    "print(\"\\n--- Step 4: Starting Fine-Tuning ---\")\n",
    "\n",
    "# This command starts the training process.\n",
    "# It will print progress and evaluation metrics.\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n--- Fine-Tuning Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 7. FINAL EVALUATION ---\n",
    "print(\"\\n--- Step 5: Final Evaluation on Validation Set ---\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Final evaluation results:\")\n",
    "print(eval_results)\n",
    "\n",
    "# Save the final model and tokenizer\n",
    "trainer.save_model(f\"{OUTPUT_DIR}/final_model\")\n",
    "print(f\"Final model saved to {OUTPUT_DIR}/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "test_dataset_tokenized = test_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test_dataset_tokenized)\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = test_dataset_tokenized['labels']\n",
    "print(\"--- Generating Confusion Matrix and Classification Report ---\")\n",
    "class_names = list(label2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Emotion')\n",
    "plt.ylabel('Manually Labeled Emotion')\n",
    "plt.title('Fine-Tuned CodeBERT Performance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "report = classification_report(true_labels, predicted_labels, target_names=class_names)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6813259,
     "sourceId": 13039923,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
